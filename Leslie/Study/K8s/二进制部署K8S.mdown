# 使用二进制部署K8S



# 一、说明

## 1.部署说明

### 1.1 部署要求

- 操作系统：Centos7.x

- 硬件配置：2C /4G以上

- 可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点

  

### 1.2 软件说明

- 操作系统：Centos7.9
- Docker：19-ce
- Kubernetes：1.18

### 1.3服务器规划

以下为多master规划，如部署单master只用3台机器即可，不需要Load Balancer和k8s-master2

| 角色                   | IP                                    | 安装组件                                                     | 配置  |
| ---------------------- | ------------------------------------- | ------------------------------------------------------------ | ----- |
| k8s-master1            | 192.168.17.100                        | kube-apiserver<br>kube-controller-manager<br>kube-scheduler<br>etcd | 2C/4G |
| k8s-master2            | 192.168.17.101                        | kube-apiserver<br>kube-controller-manager<br>kube-scheduler  | 2C/4G |
| k8s-node1              | 192.168.17.102                        | kubelet<br>kube-proxy<br>docker<br>etcd                      | 2C/4G |
| k8s-node2              | 192.168.17.103                        | kubelet<br>kube-proxy<br>etcd                                | 2C/4G |
| Load Balancer (master) | 192.168.17.104<br>192.168.17.106(vip) | Nginx(master)                                                | 1C/1G |
| Load Balancer (backup) | 192.168.17.105                        | Nginx                                                        | 1C/1G |

### 1.4 部署架构图

本次部署先部署一套单master架构，再扩容至多master。如果机器不够可以部署单master集群（只用3台机器即可）

单master架构：

![image-20210302152522163](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302152522163.png)

多master架构：

![image-20210302152645058](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302152645058.png)





## 2.系统初始化

对所有安装k8s的服务器系统进行初始化

```shell
# 关闭防火墙：
systemctl stop firewalld
systemctl disable firewalld

# 关闭selinux：
setenforce 0 # 临时
sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久

# 关闭swap：
swapoff -a  # 临时
sed -ri 's/.*swap.*/#&/' /etc/fstab  # 永久

# 同步系统时间（可以访问外网才能同步，如果没有外网请使用其它方式同步。如内网时间服务器）：
ntpdate time.windows.com
# 如果服务器时区不对，可以修改一下
timedatectl set-timezone 'Asia/Shanghai'

# 添加hosts：
vim /etc/hosts
192.168.17.100 k8s-master1
192.168.17.101 k8s-master2
192.168.17.102 k8s-node1
192.168.17.103 k8s-node2

# 修改主机名：
hostnamectl set-hostname <填写主机名>
```



# 二、Etcd部署

## 1.etcd简介

Etcd 是一个分布式键值存储系统，Kubernetes使用Etcd进行数据存储，所以先准备一个Etcd数据库，

为解决Etcd单点故障，应采用集群方式部署，这里使用3台组建集群，可容忍1台机器故障，当然，你也

可以使用5台组建集群，可容忍2台机器故障。



## 2.准备cfssl证书生成工具

cfssl是一个开源的证书管理工具，使用json文件生成证书，相比openssl更方便使用。

找任意一台服务器操作，这里用Master节点。

```shell
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 
chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64 
mv cfssl_linux-amd64 /usr/local/bin/cfssl 
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson 
mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo
```

## 3.生成etcd证书

### 3.1 创建工作目录：

```shell
mkdir -p ~/TLS/{etcd,k8s} 
cd TLS/etcd
```

### 3.2 自签CA证书：

```shell
cat > ca-config.json <<EOF 
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "www": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF

cat > ca-csr.json << EOF
{
    "CN": "etcd CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing"
        }
    ]
}
EOF
```

### 3.3 生成证书：

```shell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

# 查看生成结果
ls *pem 
ca-key.pem ca.pem
```

### 3.4 签发Etcd HTTPS证书：

注意：hosts字段中IP为所有etcd节点的集群内部通信IP，一个都不能少！为了方便后期扩容可以多写几个预留的IP。

```shell
cat > server-csr.json << EOF
{
    "CN": "etcd",
    "hosts": [
        "192.168.17.100",
        "192.168.17.102",
        "192.168.17.103"
        ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing"
        }
    ]
}
EOF
```

生成证书：

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server

# 查看
ls server*pem 
server-key.pem server.pem
```



## 3. 部署etcd

git上下载安装包：

地址：https://github.com/etcd-io/etcd/releases/download/v3.4.9/etcd-v3.4.9-linux-amd64.tar.gz

```shell
# 创建目录
mkdir -p /opt/etcd/{bin,cfg,ssl}
# 解压安装包
tar zxvf etcd-v3.4.9-linux-amd64.tar.gz
# 进入解压目录
mv etcd-v3.4.9-linux-amd64/{etcd,etcdctl} /opt/etcd/bin/

# 把刚生成的证书拷贝到ssl目录下
cp ../TLS/etcd/ca*pem ../TLS/etcd/server*pem /opt/etcd/ssl/

# 把修改过的etcd目录拷贝到etcd三个节点服务器上：
scp -r /opt/etcd root@192.168.17.100:/opt/
scp -r /opt/etcd root@192.168.17.102:/opt/
scp -r /opt/etcd root@192.168.17.103:/opt/
```

## 4. 修改配置文件

修改三个节点配置文件,**注意名称和ip**:

- 192.168.17.100

```shell
cat > /opt/etcd/cfg/etcd.conf << EOF
ETCD_NAME="etcd-1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.17.100:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.17.100:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.17.100:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.17.100:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://192.168.17.100:2380,etcd-2=https://192.168.17.102:2380,etcd-3=https://192.168.17.103:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
```

- 192.168.17.102

```shell
cat > /opt/etcd/cfg/etcd.conf << EOF
ETCD_NAME="etcd-2"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.17.102:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.17.102:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.17.102:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.17.102:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://192.168.17.100:2380,etcd-2=https://192.168.17.102:2380,etcd-3=https://192.168.17.103:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
```

- 192.168.17.103

```shell
cat > /opt/etcd/cfg/etcd.conf << EOF
ETCD_NAME="etcd-3"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.17.103:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.17.103:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.17.103:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.17.103:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://192.168.17.100:2380,etcd-2=https://192.168.17.102:2380,etcd-3=https://192.168.17.103:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
```

配置文件参数说明：

- ETCD_NAME：节点名称，集群中唯一

- ETCD_DATA_DIR：数据目录

- ETCD_LISTEN_PEER_URLS：集群通信监听地址

- ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址

- ETCD_INITIAL_ADVERTISE_PEER_URLS：集群通告地址

- ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址

- ETCD_INITIAL_CLUSTER：集群节点地址

- ETCD_INITIAL_CLUSTER_TOKEN：集群Token 

- ETCD_INITIAL_CLUSTER_STATE：加入集群的当前状态，new是新集群，existing表示加入已有集群

## 5.	启动服务并验证

把etcd启动加入systemd启动

```shell
# 在每台etcd服务器执行
cat > /usr/lib/systemd/system/etcd.service << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=/opt/etcd/cfg/etcd.conf
ExecStart=/opt/etcd/bin/etcd \
        --cert-file=/opt/etcd/ssl/server.pem \
        --key-file=/opt/etcd/ssl/server-key.pem \
        --peer-cert-file=/opt/etcd/ssl/server.pem \
        --peer-key-file=/opt/etcd/ssl/server-key.pem \
        --trusted-ca-file=/opt/etcd/ssl/ca.pem \
        --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

EOF
```



启动etcd

```shell
# 重新加载配置系统启动文件
systemctl  daemon-reload
# 启动etcd，第一台启动会卡住，其它节点启动后就整出来了
systemctl start etcd
# 加入开机启动
systemctl enable etcd
```

验证是否正常：

```shell
# 随便一台etcd节点执行以下命令
ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.17.100:2379,https://192.168.17.102:2379,https://192.168.17.103:2379" endpoint health
```

出现以下信息说明集群部署成功：

![image-20210222100748859](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210222100748859.png)

如果有问题先看日志：

/var/log/messages 或  journalctl -u etcd



# 三、K8S Master节点部署



## 1.自签CA证书

和etcd一样使用cfssl工具进行生成：

自签ca证书

```shell
# 进入目录
cd TLS/k8s
cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF

cat > ca-csr.json << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF


# 生成证书
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
# 查看生成的证书
ls *.pem
```

## 2. **kube-apiserver HTTPS**证书

```shell
cd TLS/k8s
# 修改server-csr.json配置，修改文件中hosts中的IP
# 注意IP为所有Master/LB/VIP IP，一个都不能少！为了方便后期扩容可以多写几个预留的IP。
cat > server-csr.json << EOF
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local",
      "192.168.17.100",
      "192.168.17.101",
      "192.168.17.104",
      "192.168.17.105",
      "192.168.17.106"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF
# 生成证书
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server
# 生成完成后查看
ls server*pem
server-key.pem  server.pem
```



## 3.下载安装包

下载地址：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#v1183

打开后选择下载包：

![image-20210222114558776](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210222114558776.png)

下载 完成后上传至master1服务器，并解压

```shell
# 创建安装目录
mkdir -p /opt/kubernetes/{bin,cfg,ssl,logs}

# 解压安装包
tar zxvf kubernetes-server-linux-amd64.tar.gz

# 复制文件至安装目录
cd kubernetes/server/bin
cp kube-apiserver kube-scheduler kube-controller-manager /opt/kubernetes/bin 
cp kubectl /usr/bin/
```
## 3. 部署kube-apiserver

### 3.1 创建配置文件

```shell
cat > /opt/kubernetes/cfg/kube-apiserver.conf <<EOF
KUBE_APISERVER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--etcd-servers=https://192.168.17.100:2379,https://192.168.17.102:2379,https://192.168.17.103:2379 \\
--bind-address=192.168.17.100 \\
--secure-port=6443 \\
--advertise-address=192.168.17.100 \\
--allow-privileged=true \\
--service-cluster-ip-range=10.0.0.0/24 \\
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\
--authorization-mode=RBAC,Node \\
--enable-bootstrap-token-auth=true \\
--token-auth-file=/opt/kubernetes/cfg/token.csv \\
--service-node-port-range=30000-32767 \\
--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \\
--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \\
--tls-cert-file=/opt/kubernetes/ssl/server.pem  \\
--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\
--client-ca-file=/opt/kubernetes/ssl/ca.pem \\
--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\
--etcd-cafile=/opt/etcd/ssl/ca.pem \\
--etcd-certfile=/opt/etcd/ssl/server.pem \\
--etcd-keyfile=/opt/etcd/ssl/server-key.pem \\
--audit-log-maxage=30 \\
--audit-log-maxbackup=3 \\
--audit-log-maxsize=100 \\
--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"
EOF
```

### 3.2 配置文件参数说明

- --logtostderr：启用日志

- ---v：日志等级

- --log-dir：日志目录

- --etcd-servers：etcd集群地址

- --bind-address：监听地址

- --secure-port：https安全端口

- --advertise-address：集群通告地址

- --allow-privileged：启用授权

- --service-cluster-ip-range：Service虚拟IP地址段

- --enable-admission-plugins：准入控制模块

- --authorization-mode：认证授权，启用RBAC授权和节点自管理

- --enable-bootstrap-token-auth：启用TLS bootstrap机制

- --token-auth-fifile：bootstrap token文件

- --service-node-port-range：Service nodeport类型默认分配端口范围

- --kubelet-client-xxx：apiserver访问kubelet客户端证书

- --tls-xxx-fifile：apiserver https证书

- --etcd-xxxfifile：连接Etcd集群证书

- --audit-log-xxx：审计日志

###   3.3 复制证书

把生成的kube-apiserver证书复制到ssl目录(注意路径):

```
cp TLS/k8s/ca*pem TLS/k8s/server*pem /opt/kubernetes/ssl/
```



### 3.4 **启用** **TLS Bootstrapping** **机制** 

​	TLS Bootstraping：Master apiserver启用TLS认证后，Node节点kubelet和kube-proxy要与kube- 

apiserver进行通信，必须使用CA签发的有效证书才可以，当Node节点很多时，这种客户端证书颁发需

要大量工作，同样也会增加集群扩展复杂度。为了简化流程，Kubernetes引入了TLS bootstraping机制

来自动颁发客户端证书，kubelet会以一个低权限用户自动向apiserver申请证书，kubelet的证书由

apiserver动态签署。所以强烈建议在Node上使用这种方式，目前主要用于kubelet，kube-proxy还是

由我们统一颁发一个证书。

TLS bootstraping 工作流程：

![image-20210222152541801](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210222152541801.png)

创建上述配置文件中token文件：

```shell
cat > /opt/kubernetes/cfg/token.csv <<-EOF
`head -c 16 /dev/urandom | od -An -t x | tr -d ' '`,kubelet-bootstrap,10001,"system:nodebootstrapper"
EOF
```

格式：token，用户名，UID，用户组

### 3.5  加入systemd管理

```shell
cat > /usr/lib/systemd/system/kube-apiserver.service << EOF 
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.conf
ExecStart=/opt/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
```

启动并设置开机启动

```shell
systemctl daemon-reload 
systemctl start kube-apiserver 
systemctl enable kube-apiserver
# 查看是否正常启动
systemctl status kube-apiserver 
# 也可以查看日志是否正常
/opt/kubernetes/logs
cat /var/log/messages|grep kube-apiserver|grep -i error
```



## 4.部署**kube-controller-manager** 

### 4.1 **创建配置文件**

```shell
cat > /opt/kubernetes/cfg/kube-controller-manager.conf << EOF
KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/opt/kubernetes/logs \\
--leader-elect=true \\
--master=127.0.0.1:8080 \\
--address=127.0.0.1 \\
--allocate-node-cidrs=true \\
--cluster-cidr=10.244.0.0/16 \\
--service-cluster-ip-range=10.0.0.0/24 \\
--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\
--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \\
--root-ca-file=/opt/kubernetes/ssl/ca.pem \\
--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\
--experimental-cluster-signing-duration=87600h0m0s"
EOF
```

参数说明：

- --master：通过本地非安全本地端口8080连接apiserver。

- --leader-elect：当该组件启动多个时，自动选举（HA）

- --cluster-signing-cert-file/cluster-signing-key-file：自动为kubelet颁发证书的CA，与apiserver保持一致

### 4.2 加入systemd管理

```shell
cat > /usr/lib/systemd/system/kube-controller-manager.service << EOF 
[Unit] 
Description=Kubernetes Controller Manager 
Documentation=https://github.com/kubernetes/kubernetes 

[Service] 
EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.conf 
ExecStart=/opt/kubernetes/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS 
Restart=on-failure 

[Install] 
WantedBy=multi-user.target 
EOF
```

```shell
# 启动并设置开机启动
systemctl daemon-reload 
systemctl start kube-controller-manager 
systemctl enable kube-controller-manager
```



## 5.部署kube-scheduler

### 5.1 创建配置文件

```shell
cat > /opt/kubernetes/cfg/kube-scheduler.conf << EOF
KUBE_SCHEDULER_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--leader-elect \
--master=127.0.0.1:8080 \
--bind-address=127.0.0.1"
EOF
```

- --master：通过本地非安全本地端口8080连接apiserver。 

- --leader-elect：当该组件启动多个时，自动选举（HA） 

### 5.2 加入systemd管理

```shell
cat > /usr/lib/systemd/system/kube-scheduler.service << EOF 
[Unit] 
Description=Kubernetes Scheduler 
Documentation=https://github.com/kubernetes/kubernetes 

[Service] 
EnvironmentFile=/opt/kubernetes/cfg/kube-scheduler.conf 
ExecStart=/opt/kubernetes/bin/kube-scheduler \$KUBE_SCHEDULER_OPTS 
Restart=on-failure 

[Install] 
WantedBy=multi-user.target 
EOF
```

```shell
# 启动并设置开机启动
systemctl daemon-reload 
systemctl start kube-scheduler 
systemctl enable kube-scheduler
```



## 6.查看集群状态

所有组件都已经启动成功，通过kubectl工具查看当前集群组件状态：

```shell
kubectl get cs
```

![image-20210223161003053](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210223161003053.png)



# 四、部署docker

## 1.使用二进制方式部署

下载地址：https://download.docker.com/linux/static/stable/x86_64/docker-19.03.9.tgz

在所有节点节点部署docker

```shell
# 解压
tar zxvf docker-19.03.9.tgz 
mv docker/* /usr/bin
```

## 2.加入systemd管理

```shell
cat > /usr/lib/systemd/system/docker.service << EOF 
[Unit] 
Description=Docker Application Container Engine 
Documentation=https://docs.docker.com 
After=network-online.target firewalld.service 
Wants=network-online.target

[Service] 
Type=notify 
ExecStart=/usr/bin/dockerd 
ExecReload=/bin/kill -s HUP $MAINPID 
LimitNOFILE=infinity 
LimitNPROC=infinity 
LimitCORE=infinity 
TimeoutStartSec=0 
Delegate=yes 
KillMode=process 
Restart=on-failure 
StartLimitBurst=3 
StartLimitInterval=60s 

[Install] 
WantedBy=multi-user.target 
EOF
```

## 3.配置文件

```shell
# 创建配置文件
mkdir /etc/docker 
cat > /etc/docker/daemon.json << EOF 
{ "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"] }
EOF
```

## 4.启动并设置开机启动

```shell
systemctl daemon-reload 
systemctl start docker 
systemctl enable docker
```



# 五、部署K8 Node节点部署

**下面还是在Master Node上操作，即同时作为Worker Node**

## 1.创建工作目录

创建工作目录：

```shell
mkdir -p /opt/kubernetes/{bin,cfg,ssl,logs}
# 进入安装包解压目录
cd kubernetes/server/bin
cp kubelet kube-proxy /opt/kubernetes/bin
```



## 2. 部署kubelet

### 1.创建配置文件

注意修改主机名

```shell
cat > /opt/kubernetes/cfg/kubelet.conf <<-'EOF'		
KUBELET_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--hostname-override=k8s-master1 \
--network-plugin=cni \
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
--config=/opt/kubernetes/cfg/kubelet-config.yml \
--cert-dir=/opt/kubernetes/ssl \
--pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2"
EOF
```

配置文件说明：

- --hostname-override：显示名称，集群中唯一 
- --network-plugin：启用CNI
- --kubeconfig：空路径，会自动生成，后面用于连接apiserver
- --bootstrap-kubeconfig：首次启动向apiserver申请证书
- --config：配置参数文件
- --cert-dir：kubelet证书生成目录
- --pod-infra-container-image：管理Pod网络容器的镜像

### 2. 配置参数文件

```shell
cat > /opt/kubernetes/cfg/kubelet-config.yml << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/kubernetes/ssl/ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF
```

### 3.生成bootstrap.kubeconfig文件

```shell
# 设置环境变量
KUBE_APISERVER="https://192.168.17.100:6443" # apiserver IP:PORT
TOKEN="991a1b627ce8a5028f74a521583a9d2d"  # 与token.csv里保持一致
cat /opt/kubernetes/cfg/token.csv
# 执行以下操作
# 生成 kubelet bootstrap kubeconfig 配置文件
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=bootstrap.kubeconfig
  
kubectl config set-credentials "kubelet-bootstrap" \
  --token=${TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kubelet-bootstrap" \
  --kubeconfig=bootstrap.kubeconfig
  
kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
```

拷贝配置文件：

```shell
cp bootstrap.kubeconfig /opt/kubernetes/cfg
```



### 4.加入systemd管理

```shell
cat > /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet.conf
ExecStart=/opt/kubernetes/bin/kubelet \$KUBELET_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

### 5. 启动并设置开机启动

```shell
systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet
```

## 

## 3.批准kubelet证书申请并加入集群

```shell
# 查看kubelet证书请求
kubectl get csr
```

![image-20210225150157659](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210225150157659.png)

```shell
# 批准申请
kubectl certificate approve node-csr-U0Rle1EfJclTSwWPD1pdguDfpqbZisDjjJYSJLBSUgQ

# 查看状态
kubectl get node
```

![image-20210225150407935](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210225150407935.png)

注：由于网络插件还没有部署，节点会没有准备就绪 NotReady



## 4.部署kube-proxy

### 1.创建配置文件

```shell
cat > /opt/kubernetes/cfg/kube-proxy.conf <<-'EOF'
KUBE_PROXY_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--config=/opt/kubernetes/cfg/kube-proxy-config.yml"
EOF
```

###  2.配置参数文件

注意修改主机名

```shell
cat > /opt/kubernetes/cfg/kube-proxy-config.yml <<-EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig
hostnameOverride: k8s-master1
clusterCIDR: 10.0.0.0/24
EOF
```

### 3. 生成kube-proxy证书

```shell
# 进入证书生成目录
cd TLS/k8s
# 配置文件
cat > kube-proxy-csr.json <<-EOF
{
    "CN": "system:kube-proxy",
    "hosts": [],
    "key": {
        "algo": "rsa",
        "size": 2048
},
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF
# 开始生成
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
# 查看生成结果
ls kube-proxy*pem
kube-proxy-key.pem  kube-proxy.pem
```

### 4.生成kubeconfig文件

```shell
# 设置环境变量,apiserver地址
KUBE_APISERVER="https://192.168.17.100:6443"
# 执行生成
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig
  
kubectl config set-credentials kube-proxy \
  --client-certificate=./kube-proxy.pem \
  --client-key=./kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
  
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# 将生成的文件拷贝到指定文件下
cp kube-proxy.kubeconfig /opt/kubernetes/cfg/
```

### 5.加入systemd管理

```shell
cat > /usr/lib/systemd/system/kube-proxy.service <<'EOF'
[Unit]
Description=Kubernetes Proxy
After=network.target
[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.kubeconfig
ExecStart=/opt/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTS
Restart=on-failure
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
EOF
```

### 6.启动并设置开机启动

```shell
systemctl daemon-reload
systemctl start kube-proxy
systemctl enable kube-proxy
```



## 5.部署CNI网络

准备文件二进制安装文件：

下载地址：https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz

```shell
# 解压二进制包并移动到默认工作目录
mkdir -p /opt/cni/bin
tar zxvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin
```

部署cni网络，这一步需要联网

```shell
# 下载yml文件
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
# 如果访问不了github，执行以下操作
echo "199.232.4.133 raw.githubusercontent.com" >> /etc/hosts

# 开始安装
kubectl apply -f kube-flannel.yml
# 查看状态
kubectl get pods -n kube-system

# 查看node节点状态
kubectl get node
```

![image-20210225155055102](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210225155055102.png)

## 6.授权apiserver访问kubelet

创建授权文件

```shell
cat > apiserver-to-kubelet-rbac.yaml <<-EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
    - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
      - pods/log
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```



## 7.新增node节点

将已部署好的Node相关文件到新节点

```shell
# 将相关文件拷贝到新节点
scp /opt/kubernetes root@192.168.17.102:/opt/

scp -r /usr/lib/systemd/system/{kubelet,kube-proxy}.service root@192.168.17.102:/usr/lib/systemd/system

scp -r /opt/cni/ root@192.168.17.102:/opt/

scp /opt/kubernetes/ssl/ca.pem root@192.168.17.102:/opt/kubernetes/ssl

```

kubelet证书和kubeconfig文件

```shell
rm -f /opt/kubernetes/cfg/kubelet.kubeconfig
rm -f /opt/kubernetes/ssl/kubelet*

#这几个文件是证书申请审批后自动生成的，每个Node不同，必须删除重新生成。
```



修改主机名：

```shell
vi /opt/kubernetes/cfg/kubelet.conf
# 修改内容
--hostname-override=k8s-node1

vi /opt/kubernetes/cfg/kube-proxy-config.yml
# 修改内容
hostnameOverride: k8s-node1
```

启动并设置开机启动

```shell
systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet
systemctl start kube-proxy
systemctl enable kube-proxy
```

在Master上批准新Node kubelet证书申请

```shell
kubectl get csr


[root@k8s-master1 ~]# kubectl get csr
NAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-40SI91BryzfIsyqFn9i8CL0Miw-kKN-MDN-COBi5RSM   2m51s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending

# 批准
kubectl certificate approve node-csr-40SI91BryzfIsyqFn9i8CL0Miw-kKN-MDN-COBi5RSM
```

查看node状态

```shell
# 如果状态异常，等一段时间后再查看
kubectl get node
```

![image-20210225163942368](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210225163942368.png)

**其它节点加入执行以上操作即可**



# 六 扩容为多Master节点

Kubernetes作为容器集群系统，通过健康检查+重启策略实现了Pod故障自我修复能力，通过调度算法实现将Pod分布式部署，并保持预期副本数，根据Node失效状态自动在其他Node拉起Pod，实现了应用层的高可用性。

针对Kubernetes集群，高可用性还应包含以下两个层面的考虑：Etcd数据库的高可用性和Kubernetes Master组件的高可用性。 而Etcd我们已经采用3个节点组建集群实现高可用，本节将对Master节点高可用进行说明和实施。

Master节点扮演着总控中心的角色，通过不断与工作节点上的Kubelet和kube-proxy进行通信来维护整个集群的健康工作状态。如果Master节点故障，将无法使用kubectl工具或者API做任何集群管理。

Master节点主要有三个服务kube-apiserver、kube-controller-manager和kube-scheduler，其中kube-controller-manager和kube-scheduler组件自身通过选择机制已经实现了高可用，所以Master高可用主要针对kube-apiserver组件，而该组件是以HTTP API提供服务，因此对他高可用与Web服务器类似，增加负载均衡器对其负载均衡即可，并且可水平扩容。



## 1.部署Master node2

Master2 与已部署的Master1所有操作一致。所以我们只需将Master1所有K8s文件拷贝过来，再修改下服务器IP和主机名启动即可。

### 1.1 创建etcd证书目录

在Master2创建etcd证书目录：

```shell
mkdir -p /opt/etcd/{bin,cfg,ssl} 
```



### 1.2 拷贝文件（Master1操作）

拷贝Master1上所有K8s文件和etcd证书到Master2：

```shell
scp -r /opt/kubernetes root@192.168.17.101:/opt
scp -r /opt/cni/ root@192.168.17.101:/opt
scp -r /opt/etcd/ssl root@192.168.17.101:/opt/etcd
scp /usr/lib/systemd/system/kube* root@192.168.17.101:/usr/lib/systemd/system
scp /usr/bin/kubectl  root@192.168.17.101:/usr/bin
```

### 1.3 删除证书文件（Master2操作）

删除kubelet证书和kubeconfig文件：

```shell
rm -f /opt/kubernetes/cfg/kubelet.kubeconfig
rm -f /opt/kubernetes/ssl/kubelet*
```

### 1.4 修改配置文件IP和主机名

修改apiserver、kubelet和kube-proxy配置文件为本地IP和主机名：

```shell
vi /opt/kubernetes/cfg/kube-apiserver.conf 
# 修改ip
--bind-address=192.168.17.101 \
--advertise-address=192.168.17.101 \

vi /opt/kubernetes/cfg/kubelet.conf
# 修改主机名
--hostname-override=k8s-master2 \

vi /opt/kubernetes/cfg/kube-proxy-config.yml
# 修改主机名
hostnameOverride: k8s-master2
```

### 1.5 启动设置开机启动

```shell
systemctl daemon-reload
systemctl start kube-apiserver
systemctl start kube-controller-manager
systemctl start kube-scheduler
systemctl start kubelet
systemctl start kube-proxy
systemctl enable kube-apiserver
systemctl enable kube-controller-manager
systemctl enable kube-scheduler
systemctl enable kubelet
systemctl enable kube-proxy
```



### 1.6 查看集群状态

```
kubectl get cs
```

![image-20210302160710794](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302160710794.png)



### 1.7  批准kubelet证书申请

```shell
 kubectl get csr
```

![image-20210302160830386](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302160830386.png)

```shell
# 准入
kubectl certificate approve node-csr-Llm9J5yJE61emfv5QuvO5ZomhTnqGXv7ZDaXKOeX8_s

# 查看node状态
kubectl get node
```

![image-20210302161053768](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302161053768.png)



## 2.部署Nginx负载均衡

### 2.1 kube-apiserver高可用架构图



![image-20210302161228989](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302161228989.png)

- 使用Nginx对apiserver实现负载均衡
- Keepalived是一个主流高可用软件，基于VIP绑定实现服务器双机热备，在上述拓扑中，Keepalived主要根据Nginx运行状态判断是否需要故障转移（偏移VIP），例如当Nginx主节点挂掉，VIP会自动绑定在Nginx备节点，从而保证VIP一直可用，实现Nginx高可用。



### 2.2 安装软件

主备两台机器安装nginx和keeplive。这里使用的yum方式安装，无法使用yum的可以使用二进制方式安装

```shell
 yum install epel-release -y
 yum install nginx keepalived -y
```

### 2.3 修改nginx配置文件

主备两台一样：

只需要修改apiserver ip即可

```shell
cat > /etc/nginx/nginx.conf << "EOF"
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;

include /usr/share/nginx/modules/*.conf;

events {
    worker_connections 1024;
}

# 四层负载均衡，为两台Master apiserver组件提供负载均衡
stream {

    log_format  main  '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent';

    access_log  /var/log/nginx/k8s-access.log  main;

    upstream k8s-apiserver {
       server 192.168.17.100:6443;   # Master1 APISERVER IP:PORT
       server 192.168.17.101:6443;   # Master2 APISERVER IP:PORT
    }

    server {
       listen 6443;
       proxy_pass k8s-apiserver;
    }
}

http {
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 2048;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;

    server {
        listen       80 default_server;
        server_name  _;

        location / {
        }
    }
}
EOF
```

### 2.4  keepalived配置文件（Nginx Master）

```
cat > /etc/keepalived/keepalived.conf << EOF
global_defs { 
   notification_email { 
     acassen@firewall.loc 
     failover@firewall.loc 
     sysadmin@firewall.loc 
   } 
   notification_email_from Alexandre.Cassen@firewall.loc  
   smtp_server 127.0.0.1 
   smtp_connect_timeout 30 
   router_id NGINX_MASTER
} 

vrrp_script check_nginx {
    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 { 
    state MASTER 
    interface ens33  # 修改为实际网卡名
    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 
    priority 100    # 优先级，备服务器设置 90 
    advert_int 1    # 指定VRRP 心跳包通告间隔时间，默认1秒 
    authentication { 
        auth_type PASS      
        auth_pass 1111 
    }  
    # 虚拟IP
    virtual_ipaddress { 
        192.168.17.104/24
    } 
    track_script {
        check_nginx
    } 
}
EOF
```

- vrrp_script：指定检查nginx工作状态脚本（根据nginx状态判断是否故障转移）
- virtual_ipaddress：虚拟IP（VIP）

nginx检查脚本

```shell
cat > /etc/keepalived/check_nginx.sh  << "EOF"
#!/bin/bash
count=$(ps -ef |grep nginx |egrep -cv "grep|$$")

if [ "$count" -eq 0 ];then
    exit 1
else
    exit 0
fi
EOF
# 添加执行权限
chmod +x /etc/keepalived/check_nginx.sh
```



### 2.5 keepalived配置文件（Nginx Backup）

```shell
cat > /etc/keepalived/keepalived.conf << EOF
global_defs { 
   notification_email { 
     acassen@firewall.loc 
     failover@firewall.loc 
     sysadmin@firewall.loc 
   } 
   notification_email_from Alexandre.Cassen@firewall.loc  
   smtp_server 127.0.0.1 
   smtp_connect_timeout 30 
   router_id NGINX_BACKUP
} 

vrrp_script check_nginx {
    script "/etc/keepalived/check_nginx.sh"
}

vrrp_instance VI_1 { 
    state BACKUP 
    interface ens33
    virtual_router_id 51 # VRRP 路由 ID实例，每个实例是唯一的 
    priority 90
    advert_int 1
    authentication { 
        auth_type PASS      
        auth_pass 1111 
    }  
    virtual_ipaddress { 
        192.168.17.106/24
    } 
    track_script {
        check_nginx
    } 
}
EOF
```

nginx检查脚本：

```shell
cat > /etc/keepalived/check_nginx.sh  << "EOF"
#!/bin/bash
count=$(ps -ef |grep nginx |egrep -cv "grep|$$")

if [ "$count" -eq 0 ];then
    exit 1
else
    exit 0
fi
EOF
# 添加执行权限
chmod +x /etc/keepalived/check_nginx.sh
```

### 2.6  启动并设置开机启动

```shell
systemctl daemon-reload
systemctl start nginx
systemctl start keepalived
systemctl enable nginx
systemctl enable keepalived
```

查看keepalived状态

![image-20210302170227058](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302170227058.png)

### 2.7 Nginx+Keepalived高可用测试

关闭主节点Nginx，测试VIP是否漂移到备节点服务器

```shell
# 主节点关闭nginx
pkill nginx
# 从节点查看vip是否漂移
ip addr
```

### 2.8 访问负载均衡器测试

找K8s集群中任意一个节点，使用curl查看K8s版本测试，使用VIP访问：

```shell
curl -k https://192.168.17.106:6443/version
```

![image-20210302172903325](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302172903325.png)

### 2.9 修改所有Worker Node连接LB VIP

虽然增加了Master2和负载均衡器，但是我们是从单Master架构扩容的，也就是说目前所有的Node组件连接都还是Master1，如果不改为连接VIP走负载均衡器，那么Master还是单点故障。

修改所有Node组件配置文件，由原来192.168.17.100修改为192.168.17.106（VIP）：

```shell
# 在所有k8s节点执行以下操作
sed -i 's#192.168.17.100:6443#192.168.17.106:6443#' /opt/kubernetes/cfg/*
systemctl restart kubelet
systemctl restart kube-proxy
```

检查节点状态：

```shell
kubectl get node
```

![image-20210302173941733](http://wxy-2021.oss-cn-shenzhen.aliyuncs.com/img/image-20210302173941733.png)





**至此一套完整的二进制K8S集群部署完成！**